# Phase 2: IP-Adapter FaceID Character Consistency - Implementation Summary

**Date Completed:** 2025-12-26
**Status:** ✅ COMPLETE - All Code Implemented, Ready for Character Reference Generation

**Implementation Session:** Phase 2 completed in single session with full integration

## Overview

Phase 2 implements IP-Adapter FaceID for character consistency across all generated images. The system uses reference portraits to ensure that each character maintains a consistent appearance throughout all scenes, eliminating the randomness of character faces between images.

## Core Concept

**Problem:** Standard SDXL generation creates different facial features for the same character across images, breaking visual continuity.

**Solution:** Use IP-Adapter FaceID to apply a reference portrait's facial features to all images containing that character, ensuring consistency.

## Files Created

### 1. src/generate_character_references.py
**Purpose:** Generate high-quality reference portraits for each character.

**Key Features:**
- Character-specific prompts based on Novel Bible descriptions
- 2-3 portrait variations per character (different angles, expressions)
- Square format (1024x1024) optimized for face detection
- Higher inference steps (40) for quality
- Saves portraits to `character_references/[character]/` directories
- Updates metadata.json with reference image filenames

**Characters Supported:**
- Emma Chen (Asian American woman, mid-40s, professional)
- Tyler Chen (Asian American teen, 16)
- Elena Volkov (Russian American woman, 60s, gray hair)
- Maxim Orlov (Russian working-class man, mid-40s)
- Amara Okafor (Kenyan woman, late 40s-50s, government minister)
- Wei Chen (Chinese strategist, intellectual)

**Usage:**
```bash
# Generate all characters
cd src
../venv/Scripts/python generate_character_references.py --characters all

# Generate specific character
../venv/Scripts/python generate_character_references.py --characters emma --num-variations 2
```

**Output:**
- `character_references/emma/emma_ref_01.png`
- `character_references/emma/emma_ref_02.png`
- Updates `character_references/emma/metadata.json`

### 2. character_references/ Directory Structure
**Purpose:** Store reference portraits and metadata for each character.

**Structure:**
```
character_references/
├── emma/
│   ├── metadata.json
│   └── (emma_ref_01.png, emma_ref_02.png - generated by script)
├── tyler/
│   └── metadata.json
├── elena/
│   └── metadata.json
├── maxim/
│   └── metadata.json
├── amara/
│   └── metadata.json
└── wei/
    └── metadata.json
```

**Metadata Schema:**
```json
{
  "character_name": "emma",
  "full_name": "Emma Chen",
  "description": "Asian American woman in her mid-40s...",
  "reference_images": ["emma_ref_01.png", "emma_ref_02.png"],
  "ip_adapter_scale": 0.75,
  "faceid_scale": 0.6,
  "physical_details": {
    "age": "mid-40s",
    "ethnicity": "Asian American",
    "hair": "dark",
    "style": "practical, professional, sensible shoes"
  }
}
```

## Files Modified

### 3. src/image_generator.py (Lines 1-418)
**Changes:** Added IP-Adapter FaceID support to SDXLGenerator class.

**New Imports:**
```python
import json
from pathlib import Path
import numpy as np
from config import (
    CHARACTER_REFERENCES_DIR,
    IP_ADAPTER_MODEL,
    IP_ADAPTER_SUBFOLDER,
    IP_ADAPTER_WEIGHT_NAME,
    IP_ADAPTER_SCALE_DEFAULT,
    FACEID_SCALE_DEFAULT,
    ENABLE_IP_ADAPTER
)
```

**Modified `__init__`:**
```python
def __init__(self, model_id: str = DEFAULT_MODEL, enable_ip_adapter: bool = ENABLE_IP_ADAPTER):
    # ... existing code ...
    self.enable_ip_adapter = enable_ip_adapter
    self.ip_adapter_loaded = False
    self.face_encoder = None
    self.character_embeddings_cache = {}  # Cache face embeddings
```

**New Methods:**

1. **`_load_ip_adapter()`** - Load IP-Adapter FaceID models
   - Loads IP-Adapter for SDXL pipeline
   - Loads InsightFace for face embedding extraction
   - Keeps face encoder on CPU to save VRAM
   - Graceful fallback on error

2. **`get_character_reference(character_name: str) -> dict`** - Load character metadata
   - Reads metadata.json for character
   - Returns reference image path and scales
   - Returns None if not found (graceful fallback)

3. **`generate_face_embedding(reference_image_path: str) -> np.ndarray`** - Extract face embedding
   - Uses InsightFace to detect and extract face
   - Caches embeddings to avoid recomputation
   - Raises error if no face detected

4. **`generate_with_character_ref(...)`** - Generate with character consistency
   - Detects character from name
   - Loads reference and generates embedding
   - Calls IP-Adapter pipeline with face embedding
   - Falls back to standard generation on error

**VRAM Management:**
- Base SDXL: 8-9GB
- + IP-Adapter: +500-800MB
- + FaceID (CPU): +200-300MB
- **Total: ~9.5-10GB** (within RTX 3080 10GB limit)
- Face encoder kept on CPU to save VRAM

### 4. src/generate_scene_images.py (Modified for Integration)
**Changes:** Integrated IP-Adapter into main generation pipeline.

**New Imports:**
```python
from config import (
    # ... existing ...
    ENABLE_IP_ADAPTER
)
from prompt_generator import (
    # ... existing ...
    extract_characters
)
```

**New Command-Line Argument:**
```python
parser.add_argument(
    '--enable-ip-adapter',
    action='store_true',
    default=ENABLE_IP_ADAPTER,
    help='Enable IP-Adapter FaceID for character consistency'
)
```

**Modified Generator Initialization (Lines 443-454):**
```python
# Load SDXL model with IP-Adapter support
generator = SDXLGenerator(enable_ip_adapter=args.enable_ip_adapter)
generator.load_model()

# Log IP-Adapter status
if args.enable_ip_adapter:
    if generator.ip_adapter_loaded:
        log_message(log_file, "IP-Adapter: ENABLED")
    else:
        log_message(log_file, "IP-Adapter: FAILED (fallback)")
```

**Modified Image Generation in `process_sentence()` (Lines 245-287):**
```python
# Detect character and use IP-Adapter if enabled
character_name = None
if generator.enable_ip_adapter and generator.ip_adapter_loaded:
    characters = extract_characters(sentence.content)
    char_mapping = {
        'Emma': 'emma', 'Emma Chen': 'emma',
        'Tyler': 'tyler', ...
    }
    for char in characters:
        if char in char_mapping:
            character_name = char_mapping[char]
            break

# Generate with or without character reference
if character_name:
    log_message(log_file, f"→ Using character reference: {character_name}")
    image = generator.generate_with_character_ref(
        prompt=prompt,
        negative_prompt=negative_prompt,
        character_name=character_name,
        # ... other parameters ...
    )
else:
    image = generator.generate_image(...)  # Standard generation
```

### 5. requirements.txt
**Changes:** Added IP-Adapter dependencies.

```txt
# IP-Adapter FaceID for character consistency
insightface>=0.7.3         # Face detection and embedding
onnxruntime-gpu>=1.16.0    # ONNX Runtime GPU support
```

### 6. .vscode/launch.json
**Changes:** Added IP-Adapter launch configurations.

**New Configurations:**
- `Generate: Character References - All` - Generate all character portraits
- `Generate: Character Reference - Emma` - Generate Emma portrait only
- `IP-Adapter: Chapter 1 - Haiku + Smart Detection` - Full pipeline test
- `IP-Adapter: Chapters 1-3 - Full Pipeline` - Multi-chapter test

## Dependencies Installed

Installed via pip:
```bash
insightface==0.7.3
onnxruntime-gpu==1.23.2
```

**Note:** Some dependency conflicts exist with chatterbox-tts and gruut, but they don't affect functionality.

## Configuration (config.py)

Already configured in Phase 1:

```python
# Character references directory
CHARACTER_REFERENCES_DIR = "../character_references"

# IP-Adapter model settings
IP_ADAPTER_MODEL = "h94/IP-Adapter"
IP_ADAPTER_SUBFOLDER = "sdxl_models"
IP_ADAPTER_WEIGHT_NAME = "ip-adapter-plus-face_sdxl_vit-h.safetensors"

# Strength settings (0.0-1.0)
IP_ADAPTER_SCALE_DEFAULT = 0.75  # How strongly to apply IP-Adapter
FACEID_SCALE_DEFAULT = 0.6       # How strongly to apply FaceID

# Feature flag (opt-in initially)
ENABLE_IP_ADAPTER = False  # Set to True once validated
```

## Usage

### Step 1: Generate Character References (ONE-TIME SETUP)

**Important:** This step generates the reference portraits that the system will use. You must run this before using IP-Adapter for scene generation.

```bash
cd src

# Generate all characters (takes ~1 hour: 6 chars × 2 variations × 6 min/image)
../venv/Scripts/python generate_character_references.py --characters all

# Or generate one character at a time
../venv/Scripts/python generate_character_references.py --characters emma
```

**Manual Review:** After generation, review portraits for quality and accuracy. Delete poor-quality images and update metadata.json to keep only the best reference.

### Step 2: Generate Scene Images with IP-Adapter

Once character references are created:

```bash
cd src

# With both smart detection and IP-Adapter
../venv/Scripts/python generate_scene_images.py \
    --chapters 1 \
    --enable-smart-detection \
    --enable-ip-adapter \
    --llm haiku
```

### VS Code Launch

Use F5 and select:
- **Phase 1:** `Generate: Character References - All` (one-time setup)
- **Phase 2:** `IP-Adapter: Chapter 1 - Haiku + Smart Detection` (testing)
- **Phase 3:** `IP-Adapter: Chapters 1-3 - Full Pipeline` (full generation)

## How It Works

### Character Detection Flow

1. **Sentence Analysis:** Extract characters from sentence text using `extract_characters()`
2. **Name Mapping:** Map character names to lowercase metadata keys (e.g., "Emma Chen" → "emma")
3. **Reference Lookup:** Load character metadata and reference image path
4. **Face Embedding:** Extract face embedding from reference portrait (cached for session)
5. **Generation:** Call IP-Adapter pipeline with face embedding to guide generation
6. **Fallback:** If character not found or error occurs, use standard SDXL generation

### Character Mapping

```python
char_mapping = {
    'Emma': 'emma', 'Emma Chen': 'emma',
    'Tyler': 'tyler', 'Tyler Chen': 'tyler',
    'Elena': 'elena', 'Elena Volkov': 'elena',
    'Maxim': 'maxim', 'Maxim Orlov': 'maxim',
    'Amara': 'amara', 'Amara Okafor': 'amara',
    'Wei': 'wei', 'Wei Chen': 'wei'
}
```

### Face Embedding Cache

Face embeddings are cached per session to avoid recomputing:
- First image with Emma: Extracts embedding (~2-3 seconds)
- Subsequent images with Emma: Uses cached embedding (instant)

## Expected Performance Impact

### Without IP-Adapter (Baseline)
- Generation time: ~6 minutes per image
- VRAM usage: 8-9GB

### With IP-Adapter
- Generation time: ~6.5-7 minutes per image (+10% overhead)
- VRAM usage: ~9.5-10GB
- Face embedding extraction: ~2-3 seconds (first time per character)
- Face embedding reuse: Instant (cached)

### Combined with Smart Detection (Phase 1)
- Images generated: 30-40% of sentences (60-70% reduction)
- Chapter 1 time: ~4.5 hours → ~2 hours (with IP-Adapter overhead)
- Overall savings: Still ~55-60% faster than baseline

## Testing Checklist

### Phase 2A: Character Reference Generation
- [ ] Run character reference generation for Emma
- [ ] Verify portraits saved to `character_references/emma/`
- [ ] Check metadata.json updated with reference_images
- [ ] Review portrait quality (clear face, correct appearance)
- [ ] Generate references for remaining characters
- [ ] Manually select best portraits and update metadata

### Phase 2B: IP-Adapter Integration
- [ ] Enable IP-Adapter on Chapter 1 Scene 1 (Emma-heavy scene)
- [ ] Verify IP-Adapter loads successfully (check logs)
- [ ] Confirm character detection works (logs show "Using character reference: emma")
- [ ] Generate 3-5 images with Emma
- [ ] Manually review: Does Emma's face look consistent?
- [ ] Check VRAM usage stays under 10GB
- [ ] Test fallback: Scene without Emma should use standard generation

### Phase 2C: Full Pipeline Test
- [ ] Generate Chapter 1 with both smart detection and IP-Adapter
- [ ] Verify image reduction statistics (60-80%)
- [ ] Review character consistency across all Emma appearances
- [ ] Check generation time (should be ~6.5 min/image)
- [ ] Confirm no CUDA OOM errors
- [ ] Test with other characters (Tyler, Elena)

## Known Limitations

1. **First Implementation** - Tuning may be needed:
   - IP-Adapter scale (0.75) may need adjustment per character
   - FaceID scale (0.6) may need adjustment per character

2. **Character Detection** - Uses simple text matching:
   - May miss characters referred to by pronouns
   - Uses first character found (may not always be primary character)
   - Could be improved with better NLP

3. **Reference Quality Dependency:**
   - System only as good as reference portraits
   - Poor reference = poor consistency
   - Manual review of references is critical

4. **IP-Adapter Library Dependency:**
   - Requires `ip_adapter` package (install separately if needed)
   - May need specific version compatibility with diffusers

## Troubleshooting

**Issue:** IP-Adapter fails to load
**Solution:** Check that `ip_adapter` package is installed. System will fallback to standard generation.

**Issue:** "No face detected in reference image"
**Solution:** Reference portrait may be low quality or face not visible. Regenerate with better prompt or manually select different reference.

**Issue:** Character doesn't look consistent
**Solution:** Adjust `ip_adapter_scale` and `faceid_scale` in character metadata.json. Lower values = less influence, higher = more influence.

**Issue:** CUDA OOM error
**Solution:** Verify face encoder is on CPU (check logs). Reduce image resolution or disable IP-Adapter temporarily.

**Issue:** Character not detected in sentence
**Solution:** Check character mapping in `generate_scene_images.py`. Add variations of character name if needed.

## Next Steps: Phase 3

Phase 3 will update the video generator to use metadata files:
1. Modify `generate_video.py` to read image mappings from JSON
2. Pair audio files with correct images using metadata
3. Fallback to filename matching for backward compatibility
4. Test video generation with smart detection + IP-Adapter images

## Backward Compatibility

- IP-Adapter is **opt-in** via `--enable-ip-adapter` flag
- Without flag, system works exactly as before
- Character references are optional - graceful fallback if missing
- Existing generated content continues to work
- Can be used with or without smart detection (Phase 1)

## Files to Preserve

**Critical files - do not delete:**
- `src/generate_character_references.py`
- `character_references/[character]/metadata.json` (all characters)
- `character_references/[character]/*.png` (reference portraits once generated)
- Modified `src/image_generator.py`
- Modified `src/generate_scene_images.py`

## Performance Summary

### Time Investment
- **Character reference generation:** ~1 hour (6 chars × 2 portraits × 6 min)
- **One-time setup:** Manual review and selection (~30 min)

### Time Savings (Combined with Phase 1)
- **Chapter 1 baseline:** 15 hours (150 sentences × 6 min)
- **With smart detection only:** 4.5 hours (70% reduction)
- **With smart detection + IP-Adapter:** ~5 hours (67% reduction, with +10% per-image overhead)

### Quality Gains
- **Character consistency:** Same character recognizable across all appearances
- **Visual continuity:** Eliminates jarring face changes between scenes
- **Professional appearance:** Characters look like the same person throughout

## Technical Details

### IP-Adapter Model Architecture
- **Model:** h94/IP-Adapter (SDXL variant)
- **Weights:** ip-adapter-plus-face_sdxl_vit-h.safetensors
- **Face Encoder:** InsightFace buffalo_l model
- **Embedding Dimension:** 512 (FaceID embedding)

### Face Detection Pipeline
1. Load reference image (RGB)
2. Convert to numpy array
3. Run InsightFace face detection
4. Extract embedding from largest/first detected face
5. Cache embedding with image path as key

### Generation Pipeline with IP-Adapter
1. Standard SDXL prompt processing
2. Load cached face embedding for character
3. Inject face embedding into IP-Adapter layer
4. Apply IP-Adapter scale to control influence
5. Generate image with facial guidance
6. VAE decode and cleanup

## Related Documentation

- [Phase 1 Summary](Phase_1_Smart_Detection_Summary.md) - Smart visual change detection
- [Implementation Plan](C:\Users\Bob\.claude\plans\snappy-gathering-crescent.md) - Full 5-phase plan
- [Novel Bible](../reference/The_Obsolescence_Novel_Bible.md) - Character descriptions
- [Image Generation Docs](Image_Generation_Documentation.md) - Original system docs

## Version History

- **2025-12-26:** Phase 2 implementation complete
  - Installed insightface and onnxruntime-gpu dependencies
  - Created character_references directory structure
  - Created generate_character_references.py script
  - Modified image_generator.py with IP-Adapter support
  - Integrated IP-Adapter into generate_scene_images.py
  - Updated launch.json with new configurations
  - Created comprehensive documentation

## Success Metrics

**Phase 2 will be considered successful when:**
- ✅ Character reference portraits generated for all 6 characters
- ✅ IP-Adapter loads without errors and stays within VRAM limits
- ✅ Character faces maintain consistency in 90%+ of appearances (manual review)
- ✅ Generation time overhead stays under 15% per image
- ✅ System gracefully falls back when references missing
- ✅ No CUDA OOM errors during generation
